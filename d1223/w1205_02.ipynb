{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25db0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait # í™”ë©´ì´ ë‚˜ì˜¬ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "from selenium.webdriver.support import expected_conditions as EC    # í™”ë©´ìƒíƒœì²´í¬\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aebd141",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bluer.co.kr/search?query=&foodTypeDetail=137&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%82%A8&food2=%ED%8C%8C%EC%8A%A4%ED%83%80\"    # íŒŒìŠ¤íƒ€\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"www.bluer.co.kr\"\n",
    "}\n",
    "browser = webdriver.Chrome()    # ì°½ì—´ê¸°\n",
    "browser.maximize_window() # ìµœëŒ€ì°½ìœ¼ë¡œ í™•ëŒ€\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í´ë” ì„¤ì • ===\n",
    "url = \"https://www.bluer.co.kr/search?query=&foodTypeDetail=137&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%82%A8&food2=%ED%8C%8C%EC%8A%A4%ED%83%80\"    # íŒŒìŠ¤íƒ€\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"www.bluer.co.kr\"\n",
    "}\n",
    "browser = webdriver.Chrome()    # ì°½ì—´ê¸°\n",
    "browser.maximize_window() # ìµœëŒ€ì°½ìœ¼ë¡œ í™•ëŒ€\n",
    "browser.get(url)\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ\n",
    "restaurants_csv = \"output/restaurants.csv\"\n",
    "times_csv = \"output/operating_times.csv\"\n",
    "images_csv = \"output/images.csv\"\n",
    "menus_csv = \"output/menus.csv\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# CSV í—¤ë” ì‘ì„±\n",
    "csv_headers = [\n",
    "    (restaurants_csv, [\"resno\",\"location_id\",\"locationDetail_id\",\"res_nm\",\"addr\",\"desc\",\"lat\",\"lng\",\"tel\"]),\n",
    "    (times_csv, [\"resno\",\"location_id\",\"locationDetail_id\",\"opno\",\"week\",\"open_time\",\"close_time\",\"desc\"]),\n",
    "    (images_csv, [\"resno\",\"location_id\",\"locationDetail_id\",\"imgno\",\"img_url\",\"img_nm\"]),\n",
    "    (menus_csv, [\"resno\",\"location_id\",\"locationDetail_id\",\"foodType_id\",\"fno\",\"fnm\",\"price\"])\n",
    "]\n",
    "for file, header in csv_headers:\n",
    "    with open(file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        csv.writer(f).writerow(header)\n",
    "\n",
    "# ì‹œì‘ resno\n",
    "resno_index = 1\n",
    "fno_index = 1\n",
    "opno_index = 1\n",
    "imgno_index = 1\n",
    "\n",
    "location_id = 1\n",
    "locationDetail_id = 1\n",
    "foodType_id = 1\n",
    "\n",
    "wait = WebDriverWait(browser, 10)\n",
    "resno_index = 1\n",
    "\n",
    "# ì§€ì˜¤ì½”ë” ìƒì„± (User-Agent ê¼­ í•„ìš”)\n",
    "# =====================\n",
    "# Google Mapsì—ì„œ ì´ë¦„+ì£¼ì†Œë¡œ ìœ„ë„, ê²½ë„ ê°€ì ¸ì˜¤ê¸°\n",
    "# =====================\n",
    "def get_lat_lng_google_tab(browser, name, address, wait_time=8):\n",
    "    \"\"\"\n",
    "    Google Mapsì—ì„œ ì‹ë‹¹ ì´ë¦„+ì£¼ì†Œë¡œ ìœ„ë„, ê²½ë„ ê°€ì ¸ì˜¤ê¸°\n",
    "    browser    : Selenium Chrome WebDriver\n",
    "    name       : ì‹ë‹¹ ì´ë¦„\n",
    "    address    : ì‹ë‹¹ ì£¼ì†Œ\n",
    "    wait_time  : ê²€ìƒ‰ í›„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    lat, lng = None, None\n",
    "    \n",
    "    def search_maps(query):\n",
    "        # ìƒˆ íƒ­ ì—´ê¸°\n",
    "        original_handle = browser.current_window_handle\n",
    "        browser.execute_script(\"window.open('about:blank');\")\n",
    "        browser.switch_to.window(browser.window_handles[-1])\n",
    "        browser.get(\"https://www.google.com/maps\")\n",
    "        try:\n",
    "            search_box = WebDriverWait(browser, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"searchboxinput\"))\n",
    "            )\n",
    "            search_box.clear()\n",
    "            search_box.send_keys(query)\n",
    "            search_box.send_keys(Keys.ENTER)\n",
    "            WebDriverWait(browser, wait_time).until(\n",
    "                lambda b: re.search(r'(@-?\\d+\\.\\d+,-?\\d+\\.\\d+|!3d-?\\d+\\.\\d+!4d-?\\d+\\.\\d+)', b.current_url)\n",
    "            )\n",
    "            time.sleep(1)\n",
    "            current_url = browser.current_url\n",
    "            # !3d37.xxx!4d127.xxx íŒ¨í„´ ìš°ì„ \n",
    "            m = re.search(r'!3d(-?\\d+\\.\\d+)!4d(-?\\d+\\.\\d+)', current_url)\n",
    "            if m:\n",
    "                return map(float, m.groups())\n",
    "            # @37.xxx,127.xxx íŒ¨í„´\n",
    "            m2 = re.search(r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)', current_url)\n",
    "            if m2:\n",
    "                return map(float, m2.groups())\n",
    "        except:\n",
    "            return None, None\n",
    "        finally:\n",
    "            # ìƒˆ íƒ­ ë‹«ê¸°\n",
    "            browser.close()\n",
    "            browser.switch_to.window(original_handle)\n",
    "        return None, None\n",
    "\n",
    "    # 1ï¸âƒ£ ì´ë¦„ + ì£¼ì†Œ\n",
    "    lat, lng = search_maps(f\"{name} {address}\")\n",
    "    \n",
    "    # 2ï¸âƒ£ ì‹¤íŒ¨í•˜ë©´ ì´ë¦„ë§Œ\n",
    "    if lat is None or lng is None:\n",
    "        print(f\"ì£¼ì†Œ ê²€ìƒ‰ ì‹¤íŒ¨, ì´ë¦„ë§Œ ì‹œë„: {name}\")\n",
    "        lat, lng = search_maps(name)\n",
    "    return lat, lng\n",
    "\n",
    "# ìš”ì¼ í•¨ìˆ˜\n",
    "DAY_ORDER = [\"ì›”\",\"í™”\",\"ìˆ˜\",\"ëª©\",\"ê¸ˆ\",\"í† \",\"ì¼\"]\n",
    "def expand_days(day_text):\n",
    "    days = []\n",
    "    parts = [p.strip() for p in day_text.split(\",\")]\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.replace(\"ìš”ì¼\", \"\")\n",
    "\n",
    "        if \"~\" in part:\n",
    "            start, end = part.split(\"~\")\n",
    "            s_idx = DAY_ORDER.index(start)\n",
    "            e_idx = DAY_ORDER.index(end)\n",
    "            if s_idx <= e_idx:\n",
    "                days.extend(DAY_ORDER[s_idx:e_idx+1])\n",
    "            else:\n",
    "                days.extend(DAY_ORDER[s_idx:] + DAY_ORDER[:e_idx+1])\n",
    "        else:\n",
    "            days.append(part)\n",
    "\n",
    "    return list(dict.fromkeys(days))  # ì¤‘ë³µ ì œê±°\n",
    "time.sleep(5)\n",
    "\n",
    "# === í¬ë¡¤ë§ ì‹œì‘ ===\n",
    "# =====================\n",
    "# í˜ì´ì§€ ë°˜ë³µ\n",
    "# =====================\n",
    "while True:\n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"list-restaurant\")))\n",
    "    soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "    lis = soup.select(\"#list-restaurant li\")\n",
    "    if not lis:\n",
    "        print(\"ì‹ë‹¹ ëª©ë¡ ì—†ìŒ\")\n",
    "        break\n",
    "\n",
    "    for li in lis:\n",
    "        title_tag = li.select_one(\"h3\")\n",
    "        if not title_tag:\n",
    "            continue\n",
    "        title = li.select_one(\"h3\").get_text(strip=True)\n",
    "        address = li.select_one(\".juso-info\").get_text(strip=True)\n",
    "        food_types = [f.get_text(strip=True) for f in li.select(\".foodtype li\")]\n",
    "        food_type_str = \", \".join(food_types)\n",
    "\n",
    "        thumb = li.select_one(\".thumb-restaurant\")\n",
    "        detail_url = \"https://www.bluer.co.kr\" + thumb[\"data-href\"]\n",
    "\n",
    "        tel = \"\"\n",
    "\n",
    "        # =====================\n",
    "        # ìƒì„¸í˜ì´ì§€ (ìƒˆ íƒ­)\n",
    "        # =====================\n",
    "        browser.execute_script(\"window.open(arguments[0]);\", detail_url)\n",
    "        browser.switch_to.window(browser.window_handles[-1])\n",
    "        time.sleep(1)\n",
    "\n",
    "        detail_soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "        # ì „í™”ë²ˆí˜¸\n",
    "        tel_tag = detail_soup.select_one(\"img[src*='icon_phone'] + a\")\n",
    "        if tel_tag:\n",
    "            tel = tel_tag.get_text(strip=True)\n",
    "\n",
    "        # ìš´ì˜ì‹œê°„\n",
    "        time_div = detail_soup.select_one(\".time-info .extra.on\")\n",
    "\n",
    "        if time_div:\n",
    "            raw_text = time_div.get_text(\" \", strip=True)\n",
    "            desc_text = raw_text  # descì—ëŠ” ì „ì²´ ì›ë¬¸ ê·¸ëŒ€ë¡œ\n",
    "\n",
    "            # ë¸Œë ˆì´í¬íƒ€ì„ ì œê±° (íŒŒì‹±ìš©)\n",
    "            cleaned = re.sub(r\"\\(.*?ë¸Œë ˆì´í¬íƒ€ì„.*?\\)\", \"\", raw_text)\n",
    "\n",
    "            # ğŸ”¥ ìš”ì¼ + ì‹œê°„ íŒ¨í„´ (ìš”ì¼ ë‹¨ì–´ í¬í•¨)\n",
    "            pattern = re.compile(\n",
    "                r\"([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼ìš”ì¼~, ]+)\\s+(\\d{1,2}:\\d{2})~(\\d{1,2}:\\d{2})\"\n",
    "            )\n",
    "\n",
    "            for m in pattern.finditer(cleaned):\n",
    "                day_text, open_t, close_t = m.groups()\n",
    "\n",
    "                # ìš”ì¼ í™•ì¥\n",
    "                days = expand_days(day_text)\n",
    "\n",
    "                for day in days:\n",
    "                    with open(times_csv, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                        csv.writer(f).writerow([\n",
    "                            resno_index,\n",
    "                            location_id,\n",
    "                            locationDetail_id,\n",
    "                            opno_index,\n",
    "                            day,\n",
    "                            open_t,\n",
    "                            close_t,\n",
    "                            desc_text\n",
    "                        ])\n",
    "                    opno_index+=1\n",
    "        # ì´ë¯¸ì§€\n",
    "        # for img in detail_soup.select(\".owl-stage img\"):\n",
    "        #     img_url = img.get(\"src\")\n",
    "        #     if img_url and img_url.startswith(\"/\"):\n",
    "        #         img_url = \"https://www.bluer.co.kr\" + img_url\n",
    "\n",
    "        #     img_nm = f\"{resno_index}_{img_url.split('/')[-1]}\"\n",
    "        #     save_path = os.path.join(\"images\", img_nm)\n",
    "\n",
    "        #     try:\n",
    "        #         r = requests.get(img_url, headers=headers)\n",
    "        #         if r.status_code == 200:\n",
    "        #             with open(save_path, \"wb\") as f:\n",
    "        #                 f.write(r.content)\n",
    "        #             with open(images_csv, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        #                 csv.writer(f).writerow([resno_index, location_id,locationDetail_id,imgno_index,save_path, img_nm])\n",
    "        #         imgno_index+=1\n",
    "        #     except:\n",
    "        #         pass\n",
    "\n",
    "        # ë©”ë‰´\n",
    "        menu_items = detail_soup.select(\".restaurant-menu-list .item-menu\")\n",
    "        seen = set()  # ì´ë¯¸ ì €ì¥í•œ ë©”ë‰´ë¥¼ ê¸°ë¡\n",
    "\n",
    "        for menu in menu_items:\n",
    "            name_tag = menu.select_one(\".title\")\n",
    "            price_tag = menu.select_one(\".price\")\n",
    "            if not (name_tag and price_tag):\n",
    "                continue\n",
    "\n",
    "            name_text = name_tag.get_text(strip=True)\n",
    "            price_text = price_tag.get_text(strip=True)\n",
    "\n",
    "            # 'ì›' ë°”ë¡œ ì• ìˆ«ìë§Œ\n",
    "            price_match = re.search(r'(\\d[\\d,]*)\\s*ì›', price_text)\n",
    "            price_value = int(price_match.group(1).replace(\",\", \"\")) if price_match else 0\n",
    "\n",
    "            # ì´ë¯¸ ì €ì¥í•œ ë©”ë‰´ë©´ skip\n",
    "            if (name_text, price_value) in seen:\n",
    "                continue\n",
    "            seen.add((name_text, price_value))\n",
    "            \n",
    "            # í•œ ë²ˆë§Œ ì €ì¥\n",
    "            with open(menus_csv, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                csv.writer(f).writerow([resno_index, location_id, locationDetail_id, foodType_id, fno_index, name_text, price_value])\n",
    "            \n",
    "            fno_index+=1\n",
    "                \n",
    "            # ìƒì„¸í˜ì´ì§€ ë¦¬ë·°\n",
    "            desc_tag = detail_soup.select_one(\"div.restaurant-review-info .content\")\n",
    "            if desc_tag:\n",
    "                desc_text = desc_tag.get_text(strip=True)\n",
    "                # ë§¨ ì•, ë§¨ ë’¤ ê³µë°± ì œê±° í›„ ì–‘ìª½ ë”°ì˜´í‘œ ì œê±°\n",
    "                food_type_str = desc_text.strip().strip('\"')\n",
    "\n",
    "        browser.close()\n",
    "        browser.switch_to.window(browser.window_handles[0])\n",
    "\n",
    "        # ì£¼ì†Œì—ì„œ ìœ„ë„, ê²½ë„ ê°€ì ¸ì˜¤ê¸°\n",
    "        # clean_addr = re.sub(r\"\\(.*?\\)\", \"\", address).strip()\n",
    "        lat, lng = get_lat_lng_google_tab(browser, title, address)\n",
    "        # time.sleep(8)  # Nominatimì€ ìš”ì²­ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ 3ì´ˆ ì‰¬ëŠ” ê²ƒì´ ì•ˆì „\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë³´ ì €ì¥\n",
    "        with open(restaurants_csv, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            csv.writer(f).writerow([\n",
    "                resno_index, location_id, locationDetail_id, title, address, food_type_str, lat, lng, tel\n",
    "            ])\n",
    "\n",
    "        print(f\"{resno_index} ì €ì¥ ì™„ë£Œ - {title}\")\n",
    "        resno_index += 1\n",
    "\n",
    "    # =====================\n",
    "    # ë‹¤ìŒ í˜ì´ì§€\n",
    "    # =====================\n",
    "    try:\n",
    "        next_btn = browser.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"#page-selection ul li.next:not(.disabled) a\"\n",
    "        )\n",
    "        next_btn.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"ë§ˆì§€ë§‰ í˜ì´ì§€\")\n",
    "        break\n",
    "\n",
    "browser.quit()\n",
    "print(\"âœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df82533",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
